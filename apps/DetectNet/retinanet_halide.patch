diff --git a/modules/dnn/include/opencv2/dnn/dnn.hpp b/modules/dnn/include/opencv2/dnn/dnn.hpp
index e99f844..197b5e0 100644
--- a/modules/dnn/include/opencv2/dnn/dnn.hpp
+++ b/modules/dnn/include/opencv2/dnn/dnn.hpp
@@ -636,6 +636,8 @@ CV__DNN_EXPERIMENTAL_NS_BEGIN
          */
         CV_WRAP int64 getPerfProfile(CV_OUT std::vector<double>& timings);
 
+        CV_WRAP void getOutputBlobs(int layerId, std::vector<Mat>& blobs);
+
     private:
         struct Impl;
         Ptr<Impl> impl;
diff --git a/modules/dnn/src/dnn.cpp b/modules/dnn/src/dnn.cpp
index bc18695..179362c 100644
--- a/modules/dnn/src/dnn.cpp
+++ b/modules/dnn/src/dnn.cpp
@@ -1143,6 +1143,18 @@ struct Net::Impl
         return it->second;
     }
 
+    void getOutputBlobs(int layerId, std::vector<Mat>& blobs)
+	{
+        MapIdToLayerData::iterator it = layers.find(layerId);
+
+        if (it == layers.end())
+            CV_Error(Error::StsObjectNotFound, format("Layer with requested id=%d not found", layerId));
+
+        for (int i = 0; i < it->second.outputBlobs.size(); i++) {
+        	blobs.push_back(it->second.outputBlobs[i]);
+        }
+	}
+
     LayerData& getLayerData(const String &layerName)
     {
         int id = getLayerId(layerName);
@@ -1256,6 +1268,7 @@ struct Net::Impl
                 baseIt = it;
                 continue;
             }
+
             // Try to do layers fusion.
             LayerData &ldBot = baseIt->second;
             Ptr<Layer> layerBot = ldBot.layerInstance;
@@ -2740,6 +2753,11 @@ std::vector<Ptr<Layer> > Net::getLayerInputs(LayerId layerId)
     return inputLayers;
 }
 
+void Net::getOutputBlobs(int layerId, std::vector<Mat>& blobs)
+{
+	impl->getOutputBlobs(layerId, blobs);
+}
+
 std::vector<String> Net::getLayerNames() const
 {
     std::vector<String> res;
diff --git a/modules/dnn/src/layers/elementwise_layers.cpp b/modules/dnn/src/layers/elementwise_layers.cpp
index 0a5ed54..4143b69 100644
--- a/modules/dnn/src/layers/elementwise_layers.cpp
+++ b/modules/dnn/src/layers/elementwise_layers.cpp
@@ -142,10 +142,23 @@ public:
     {
 #ifdef HAVE_HALIDE
         Halide::Buffer<float> input = halideBuffer(inputs[0]);
-        Halide::Var x("x"), y("y"), c("c"), n("n");
-        Halide::Func top = (this->name.empty() ? Halide::Func() : Halide::Func(this->name));
-        func.attachHalide(input(x, y, c, n), top);
-        return Ptr<BackendNode>(new HalideBackendNode(top));
+        int inputDims = input.dimensions();
+
+        if (inputDims == 3) {
+            printf("!!! Input dimensions %d. Use 3-D buffer !!! \n", input.dimensions());
+            Halide::Var x("x"), c("c"), n("n");
+            Halide::Func top = (this->name.empty() ? Halide::Func() : Halide::Func(this->name));
+            Halide::Expr halide_input = input(x,c,n);
+            func.attachHalide(halide_input, top);
+            return Ptr<BackendNode>(new HalideBackendNode(top));
+        } else {
+            Halide::Var x("x"), y("y"), c("c"), n("n");
+            Halide::Func top = (this->name.empty() ? Halide::Func() : Halide::Func(this->name));
+            Halide::Expr halide_input = input(x,y,c,n);
+            func.attachHalide(halide_input, top);
+            return Ptr<BackendNode>(new HalideBackendNode(top));
+        }
+
 #endif  // HAVE_HALIDE
         return Ptr<BackendNode>();
     }
diff --git a/modules/dnn/src/op_halide.cpp b/modules/dnn/src/op_halide.cpp
index c96971b..7863094 100644
--- a/modules/dnn/src/op_halide.cpp
+++ b/modules/dnn/src/op_halide.cpp
@@ -168,18 +168,25 @@ void getCanonicalSize(const MatShape& shape, int* width, int* height,
                       int* channels, int* batch)
 {
     const int dims = shape.size();
-    CV_Assert(dims == 2 || dims == 4);
+    CV_Assert(dims == 2 || dims == 4 || dims == 3);
     *batch = shape[0];
-    *channels = shape[1];
     if (dims == 4)
     {
         *width = shape[3];
         *height = shape[2];
+        *channels = shape[1];
+    } else if (dims == 3)
+    {
+        printf("shape: %d %d %d %d\n", shape[0], shape[1], shape[2], shape[3]);
+        *width = shape[1];
+        *height = 1;
+        *channels = shape[2];
     }
     else
     {
         *width = 1;
         *height = 1;
+        *channels = shape[1];
     }
 }
 
diff --git a/modules/dnn/src/tensorflow/tf_graph_simplifier.cpp b/modules/dnn/src/tensorflow/tf_graph_simplifier.cpp
index a766d2a..351be57 100644
--- a/modules/dnn/src/tensorflow/tf_graph_simplifier.cpp
+++ b/modules/dnn/src/tensorflow/tf_graph_simplifier.cpp
@@ -677,7 +677,26 @@ void RemoveIdentityOps(tensorflow::GraphDef& net)
         if (type == "Identity" || type == "Dropout") {
             identity_ops_idx.push_back(li);
             identity_ops[layer.name()] = layer.input(0);
-        }
+        } else if (type == "StridedSlice") {
+             /* Temporary hack for stridedslice + expanddim that behaves like identity ops (In retinanet P4 and P5 upsample layer) */
+            int next_li = li + 1;
+            if (next_li < layersCount) {
+                printf("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n");
+                printf("!!!!!!!!!! HACK %s, %d!!!!!!!!!!!!!!!!!!!\n", __FILE__,__LINE__);
+                printf("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n");
+                const tensorflow::NodeDef &next_layer = net.node(next_li);
+                String type = next_layer.op();
+                if (type == "ExpandDims") {
+                    identity_ops_idx.push_back(li);
+                    identity_ops[layer.name()] = layer.input(0);
+                    identity_ops_idx.push_back(next_li);
+                    identity_ops[next_layer.name()] = layer.input(0);
+                    printf("identify_ops[%s] = %s\n", layer.name().c_str(), layer.input(0).c_str());
+                    printf("identify_ops[%s] = %s\n", next_layer.name().c_str(), layer.input(0).c_str());
+                }
+
+            }
+        } 
     }
 
     for (int li = 0; li < layersCount; li++)
